p8105_hw2_hp2661
================
Huizhong Peng
2024-09-27

# Problem 1

Import the CSV file and clean the data.

``` r
dat_NYC = read_csv(
  file = "./NYC_Transit_Subway_Entrance_And_Exit_Data.csv"
  ) |> 
  janitor::clean_names() |> 
  select(line, 
         station_name, 
         station_latitude, 
         station_longitude, 
         starts_with("route"), 
         entry, 
         vending, 
         entrance_type, 
         ada) |> 
  mutate(
    entry = ifelse(entry == "YES", TRUE, FALSE), 
    across(route8:route11, as.character)
    ) |> 
  distinct() # remove duplicate rows

skimr::skim(dat_NYC)
```

|                                                  |         |
|:-------------------------------------------------|:--------|
| Name                                             | dat_NYC |
| Number of rows                                   | 684     |
| Number of columns                                | 19      |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |         |
| Column type frequency:                           |         |
| character                                        | 15      |
| logical                                          | 2       |
| numeric                                          | 2       |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |         |
| Group variables                                  | None    |

Data summary

**Variable type: character**

| skim_variable | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:--------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| line          |         0 |          1.00 |   5 |  17 |     0 |       36 |          0 |
| station_name  |         0 |          1.00 |   4 |  39 |     0 |      356 |          0 |
| route1        |         0 |          1.00 |   1 |   2 |     0 |       24 |          0 |
| route2        |       312 |          0.54 |   1 |   2 |     0 |       20 |          0 |
| route3        |       511 |          0.25 |   1 |   2 |     0 |       18 |          0 |
| route4        |       566 |          0.17 |   1 |   1 |     0 |       13 |          0 |
| route5        |       590 |          0.14 |   1 |   1 |     0 |       12 |          0 |
| route6        |       636 |          0.07 |   1 |   1 |     0 |        7 |          0 |
| route7        |       652 |          0.05 |   1 |   2 |     0 |        7 |          0 |
| route8        |       664 |          0.03 |   1 |   1 |     0 |        3 |          0 |
| route9        |       673 |          0.02 |   1 |   1 |     0 |        2 |          0 |
| route10       |       677 |          0.01 |   1 |   1 |     0 |        1 |          0 |
| route11       |       677 |          0.01 |   1 |   1 |     0 |        1 |          0 |
| vending       |         0 |          1.00 |   2 |   3 |     0 |        2 |          0 |
| entrance_type |         0 |          1.00 |   4 |   9 |     0 |        7 |          0 |

**Variable type: logical**

| skim_variable | n_missing | complete_rate | mean | count              |
|:--------------|----------:|--------------:|-----:|:-------------------|
| entry         |         0 |             1 | 0.89 | TRU: 611, FAL: 73  |
| ada           |         0 |             1 | 0.26 | FAL: 505, TRU: 179 |

**Variable type: numeric**

| skim_variable     | n_missing | complete_rate |   mean |   sd |     p0 |    p25 |    p50 |    p75 |   p100 | hist  |
|:------------------|----------:|--------------:|-------:|-----:|-------:|-------:|-------:|-------:|-------:|:------|
| station_latitude  |         0 |             1 |  40.73 | 0.07 |  40.58 |  40.68 |  40.73 |  40.77 |  40.90 | ▂▆▇▃▂ |
| station_longitude |         0 |             1 | -73.94 | 0.06 | -74.03 | -73.99 | -73.96 | -73.91 | -73.76 | ▇▇▅▂▁ |

This dataset contains 19 variables in total, including 15 character
variables (Line, station name, Route 1-11, vending and entrance type), 2
numeric variables (station latitude/longitude), and 2 logical variables
(entry and ADA compliance). I rename variables, selected the key
columns, converted the character variable “Entry” to a logical variable,
and converted the numeric variables route8-11 to the character
variables, and removed the duplicate rows. There is no need to remove
NA. The dimension of the resulting dataset is 684 × 19, and they are
tidy as each column represents a variable and each row represents one
observation.

**Question 1**

``` r
station = dat_NYC |> 
  distinct(station_name, line, .keep_all = TRUE)

nrow(station)
```

There are 465 distinct stations.

**Question 2**

``` r
station |> 
  filter(ada == TRUE) |> 
  nrow()
```

84 stations are ADA compliant.

**Question 3**

``` r
dat_NYC |> 
  filter(vending == "NO") |> 
  summarize(proportion = mean(entry))
```

38.46% of station entrances / exits without vending allow entrance.

**Question 4**

Reformat data so that route number and route name are distinct
variables.

``` r
dat_NYC_longer = dat_NYC |> 
  pivot_longer(
    cols = starts_with("route"), 
    names_to = "route_number", 
    values_to = "route_name", 
    values_drop_na = TRUE
  ) |> 
  mutate(
    route_number = parse_number(route_number)
  )

head(dat_NYC_longer, 5)
```

    ## # A tibble: 5 × 10
    ##   line     station_name station_latitude station_longitude entry vending
    ##   <chr>    <chr>                   <dbl>             <dbl> <lgl> <chr>  
    ## 1 4 Avenue 25th St                  40.7             -74.0 TRUE  YES    
    ## 2 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ## 3 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ## 4 4 Avenue 45th St                  40.6             -74.0 TRUE  YES    
    ## 5 4 Avenue 53rd St                  40.6             -74.0 TRUE  YES    
    ## # ℹ 4 more variables: entrance_type <chr>, ada <lgl>, route_number <dbl>,
    ## #   route_name <chr>

``` r
station_A = dat_NYC_longer |>
  distinct(station_name, line, .keep_all = TRUE) |> 
  filter(route_name == "A")

nrow(station_A)

station_A |> 
  filter(ada == TRUE) |> 
  nrow()
```

60 distinct stations serve the A train. Of these stations that serve the
A train, 17 stations are ADA compliant.

# Problem 2

Import the dataset and clean the sheet.

``` r
MR = read_excel(
  path = "./202309 Trash Wheel Collection Data.xlsx", 
  sheet = 1, 
  range = "A2:N586"
  ) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    sports_balls = as.integer(round(sports_balls, 0)), 
    source = "MR", 
    year = as.numeric(year)
  )

Professor = read_excel(
  path = "./202309 Trash Wheel Collection Data.xlsx", 
  sheet = 2, 
  range = "A2:M108"
  ) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    source = "Professor"
  )

Gwynnda = read_excel(
  path = "./202309 Trash Wheel Collection Data.xlsx", 
  sheet = 4, 
  range = "A2:L157"
  ) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    source = "Gwynnda"
  )

# combine the datasets

trash_tidy = bind_rows(MR, Professor, Gwynnda)

str(trash_tidy)
```

    ## tibble [845 × 15] (S3: tbl_df/tbl/data.frame)
    ##  $ dumpster          : num [1:845] 1 2 3 4 5 6 7 8 9 10 ...
    ##  $ month             : chr [1:845] "May" "May" "May" "May" ...
    ##  $ year              : num [1:845] 2014 2014 2014 2014 2014 ...
    ##  $ date              : POSIXct[1:845], format: "2014-05-16" "2014-05-16" ...
    ##  $ weight_tons       : num [1:845] 4.31 2.74 3.45 3.1 4.06 2.71 1.91 3.7 2.52 3.76 ...
    ##  $ volume_cubic_yards: num [1:845] 18 13 15 15 18 13 8 16 14 18 ...
    ##  $ plastic_bottles   : num [1:845] 1450 1120 2450 2380 980 1430 910 3580 2400 1340 ...
    ##  $ polystyrene       : num [1:845] 1820 1030 3100 2730 870 2140 1090 4310 2790 1730 ...
    ##  $ cigarette_butts   : num [1:845] 126000 91000 105000 100000 120000 90000 56000 112000 98000 130000 ...
    ##  $ glass_bottles     : num [1:845] 72 42 50 52 72 46 32 58 49 75 ...
    ##  $ plastic_bags      : num [1:845] 584 496 1080 896 368 ...
    ##  $ wrappers          : num [1:845] 1162 874 2032 1971 753 ...
    ##  $ sports_balls      : int [1:845] 7 5 6 6 7 5 3 6 6 7 ...
    ##  $ homes_powered     : num [1:845] 0 0 0 0 0 0 0 0 0 0 ...
    ##  $ source            : chr [1:845] "MR" "MR" "MR" "MR" ...

There are 845 observations and 15 variables including 12 numeric
variables (eg., weight_tons, cigarette_butts, homes_powered) and 2
character variables (month, source).

``` r
# total weight of trash collected by Professor Trash Wheel

trash_tidy |> 
  filter(
    source == "Professor"
  ) |> 
  summarize(
    total_weight_prof = sum(weight_tons, na.rm = TRUE)
  )

# total number of cigarette butts collected by Gwynnda in June of 2022

trash_tidy |> 
  filter(
    source == "Gwynnda",
    year == 2022, 
    month == "June"
  ) |> 
  summarize(
    total_ciga_gwy = sum(cigarette_butts, na.rm = TRUE)
  )
```

The total weight of trash collected by Professor Trash Wheel is 216.26
tons. The total number of cigarette butts collected by Gwynnda in June
of 2022 is 18120.

# Problem 3

Import the datasets and clean the data.

``` r
dat_bakers = read_csv("./gbb_datasets/bakers.csv") |> 
  janitor::clean_names() |> 
  separate(
    baker_name, 
    into = c("baker_first_name", "baker_last_name"), 
    sep = " "
  ) |> 
  rename(baker = baker_first_name)

dat_bakes = read_csv("./gbb_datasets/bakes.csv") |> 
  janitor::clean_names() |> 
  mutate(
    baker = ifelse(baker == "\"Jo\"", "Jo", baker)
  )

dat_results = read_csv("./gbb_datasets/results.csv", skip = 2) |> 
  janitor::clean_names() |> 
  filter(!is.na(result)) |> 
  mutate(
    baker = ifelse(baker == "Joanne", "Jo", baker)
  )

# check 

anti_join(dat_bakers, dat_results)
anti_join(dat_bakers, dat_bakes)

# merge

dat_merge1 = left_join(dat_results, dat_bakes) 
dat_merge2 = left_join(dat_merge1, dat_bakers)

# organize cols

dat_merge_f = dat_merge2 |> 
  distinct() |> 
  select(series, episode, baker, signature_bake, technical, result, 
         show_stopper, everything())
```

``` r
# summary

skimr::skim(dat_merge_f)
```

|                                                  |             |
|:-------------------------------------------------|:------------|
| Name                                             | dat_merge_f |
| Number of rows                                   | 710         |
| Number of columns                                | 11          |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |             |
| Column type frequency:                           |             |
| character                                        | 7           |
| numeric                                          | 4           |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |             |
| Group variables                                  | None        |

Data summary

**Variable type: character**

| skim_variable    | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:-----------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| baker            |         0 |          1.00 |   2 |  10 |     0 |      107 |          0 |
| signature_bake   |       162 |          0.77 |   8 | 123 |     0 |      545 |          0 |
| result           |         0 |          1.00 |   2 |  10 |     0 |        7 |          0 |
| show_stopper     |       164 |          0.77 |   3 | 200 |     0 |      529 |          0 |
| baker_last_name  |         0 |          1.00 |   3 |  17 |     0 |      117 |          0 |
| baker_occupation |         0 |          1.00 |   5 |  42 |     0 |      106 |          0 |
| hometown         |         0 |          1.00 |   5 |  41 |     0 |       96 |          0 |

**Variable type: numeric**

| skim_variable | n_missing | complete_rate |  mean |    sd |  p0 | p25 | p50 | p75 | p100 | hist  |
|:--------------|----------:|--------------:|------:|------:|----:|----:|----:|----:|-----:|:------|
| series        |         0 |          1.00 |  5.85 |  2.72 |   1 |   4 |   6 |   8 |   10 | ▅▇▇▇▇ |
| episode       |         0 |          1.00 |  4.26 |  2.61 |   1 |   2 |   4 |   6 |   10 | ▇▆▅▃▂ |
| technical     |        14 |          0.98 |  4.84 |  2.98 |   1 |   2 |   4 |   7 |   13 | ▇▅▅▂▁ |
| baker_age     |         0 |          1.00 | 36.31 | 12.85 |  17 |  28 |  32 |  42 |   71 | ▅▇▃▁▂ |

``` r
# write csv

write_csv(dat_merge_f, "./gbb_datasets/merge_data.csv")
```

**Cleaning progress:**

- Use reasonable variable names in all datasets.

- In dataset “dat_bakers”, the variable “baker_name” had different name
  and form from other two datasets, so separate this variable into two
  columns “baker_first_name” and “baker_last_name”, and rename the
  “baker_first_name” as “baker”.

- In dataset “dat_bakes”, “Jo” in variable “baker” had a different name
  form from other datasets, so remove the quotation marks around “Jo”.

- In dataset “dat_results”, omit the first two useless rows and change
  “Joanne” in column “baker” to “Jo”.

**Briefly Discussion:**

There are 710 rows and 11 columns, including 7 character variables (eg.,
baker, result, show_stopper) and 4 numeric variables (eg., series,
episode, technical). Variables “signature_bake”, “show_stopper” and
“technical” have missing values.

**Star bakers / Winners:**

``` r
star_baker = dat_merge_f |> 
  filter(series >= 5 & series <= 10 & result %in% c("STAR BAKER", "WINNER")) |> 
  select(series, episode, result, baker) |> 
  arrange(series, episode)

star_baker
```

    ## # A tibble: 60 × 4
    ##    series episode result     baker  
    ##     <dbl>   <dbl> <chr>      <chr>  
    ##  1      5       1 STAR BAKER Nancy  
    ##  2      5       2 STAR BAKER Richard
    ##  3      5       3 STAR BAKER Luis   
    ##  4      5       4 STAR BAKER Richard
    ##  5      5       5 STAR BAKER Kate   
    ##  6      5       6 STAR BAKER Chetna 
    ##  7      5       7 STAR BAKER Richard
    ##  8      5       8 STAR BAKER Richard
    ##  9      5       9 STAR BAKER Richard
    ## 10      5      10 WINNER     Nancy  
    ## # ℹ 50 more rows

- Nadiya obtained 3 “star baker” in series 6.
- Candice obtained 3 “star baker” in series 7.
- Sophie obtained 2 “star baker” in series 8.
- Rahul obtained 2 “star baker” in series 9.
- **It is not surprising that they are winners.**  
    
- Nancy obtained 1 “star baker” in series 5.
- David never obtained “star baker” in series 10.
- **It is hard to predict that they would be the winners.**

  

**Viewers:**

``` r
dat_viewers = read_csv("./gbb_datasets/viewers.csv") |> 
  janitor::clean_names()
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
print(dat_viewers, n = 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

``` r
average_1 = dat_viewers |> 
  pull(series_1) |> 
  mean(na.rm = T)
average_5 = dat_viewers |> 
  pull(series_5) |> 
  mean(na.rm = T)
```

``` r
average_1
average_5
```

The average viewership in Season 1 is 2.77.  
The average viewership in Season 5 is 10.04.
